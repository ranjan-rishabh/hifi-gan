{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "0.12.0\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Rate: 16000\n",
      "<class 'torchaudio.models.wav2vec2.model.Wav2Vec2Model'>\n"
     ]
    }
   ],
   "source": [
    "bundle = torchaudio.pipelines.WAV2VEC2_BASE\n",
    "\n",
    "print(\"Sample Rate:\", bundle.sample_rate)\n",
    "\n",
    "model = bundle.get_model().to(device)\n",
    "\n",
    "print(model.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_files/wavs/LJ001-0001.wav',\n",
       " 'train_files/wavs/LJ001-0002.wav',\n",
       " 'train_files/wavs/LJ001-0003.wav',\n",
       " 'train_files/wavs/LJ001-0004.wav',\n",
       " 'train_files/wavs/LJ001-0005.wav',\n",
       " 'train_files/wavs/LJ001-0006.wav',\n",
       " 'train_files/wavs/LJ001-0007.wav',\n",
       " 'train_files/wavs/LJ001-0008.wav',\n",
       " 'train_files/wavs/LJ001-0009.wav',\n",
       " 'train_files/wavs/LJ001-0010.wav']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "input_training_file = 'train_files/training.txt'\n",
    "input_wavs_dir = 'train_files/wavs'\n",
    "with open(input_training_file, 'r', encoding='utf-8') as fi:\n",
    "        training_files = [os.path.join(input_wavs_dir, x.split('|')[0] + '.wav')\n",
    "                          for x in fi.read().split('\\n') if len(x) > 0]\n",
    "\n",
    "training_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 482, 768])\n",
      "torch.Size([1, 94, 768])\n",
      "torch.Size([1, 483, 768])\n",
      "torch.Size([1, 256, 768])\n",
      "torch.Size([1, 405, 768])\n",
      "torch.Size([1, 283, 768])\n",
      "torch.Size([1, 419, 768])\n",
      "torch.Size([1, 88, 768])\n",
      "torch.Size([1, 377, 768])\n",
      "torch.Size([1, 440, 768])\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'wav2vec'\n",
    "\n",
    "for file in training_files:\n",
    "    waveform, sample_rate = torchaudio.load(file)\n",
    "    waveform = waveform.to(device)\n",
    "\n",
    "    if sample_rate != bundle.sample_rate:\n",
    "        waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "\n",
    "    # with torch.inference_mode():\n",
    "    #     features, _ = model.extract_features(waveform)\n",
    "\n",
    "    res, _ = model(waveform)\n",
    "\n",
    "    output_file = os.path.join(output_dir, file.split('/')[-1].split('.')[0] + '.pt')\n",
    "\n",
    "    torch.save(res, output_file)\n",
    "    print(res.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav2vec/LJ001-0001.pt\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'train_files/wavs/LJ001-0001.wav'\n",
    "output_dir1 = 'wav2vec'\n",
    "print(os.path.join(output_dir1, os.path.splitext(os.path.split(output_dir)[-1])[0] + '.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 482, 768])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "file = 'wav2vec/LJ001-0001.pt'\n",
    "\n",
    "r = torch.load(file)\n",
    "\n",
    "print(r.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = r.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 482])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = torch.transpose(r, 0, 1)\n",
    "r.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 482])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.unsqueeze(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dfccba16f8a296d2c58eb5bd6c259299bdb9c4ac9002f05fde10008e520e52e0"
  },
  "kernelspec": {
   "display_name": "Python 3.6.15 ('hifi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
